# backprop_doc
This is a personal note around the derivation of backorop.

Usually, backprop is explained along the error minimization. 
However, in this note, I described from the view point that backprop is some more general gradient calculation
algorithm which efficiently calculate the derivatives for the output-wise-sum type variables 
such as the entropy of the output units of the multi-layer perceptrons.

Copyright 2015 (c) Naoto Yoshida All Rights Reserved
